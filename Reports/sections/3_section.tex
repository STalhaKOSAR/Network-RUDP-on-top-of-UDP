\subsection{Common Parts for All Nodes}
Since we are asked to design our own protocol that should be reusable for any topology or any network configuration based bandwidth, delay, and packet size, we are passing necessary inputs (IPs, port numbers and input file) while executing the scripts. How to done that will be explained in corresponding sub-sections of nodes. We have taken ordinary care while writing all of our functions in try catch block to detect any error in our code easily, which we might faced with while implementing the protocol and beyond that. Note that below sub-sections are methodology to overview of our codes. Mechanisms use to provide reliability are checksum, Ack/nAck, retransmission and sequence number. How this mechanisms work described in below-subsections. 
\newpage
\subsection{Source Node}
We pass IP address, and port number of broker and input file that is going to be sent over network as argument while executing the script, then in the code we are placing them in necessary places. To see how to execute $source.py$ file please check $README.md$.
In the requirements ,we are given maximum packet size as $1000$  bytes. Our readFile() function is reading the data as $996$ $(MAX\_PACKET\_SIZE - 4)$ bytes (MAX\_PACKET\_SIZE depends on designer of applying topology) since header will be added in broker. As soon as the sendFile() function starts, we are getting time from \href{https://developers.google.com/time/}{NTP Server of Google} to synchronization of the nodes and write the time to time.txt file(just for working on local machine). We are not going to send time in the packet since in this part of this experiment is not focusing on delays. Therefore, the time that written first at time.txt is the time of first packets' sending time to broker. In source node we create TCP connection as: 

self.localSocket = socket.socket(socket.AF\_INET,\\
socket.SOCK\_STREAM)\\
where AF\_INET is for IPV4, and SOCK\_STREAM is for TCP. \\
Since in this experiment we need to construct a network to reliable data transfer, we don't need to apply any protocol that is written by us to TCP part. Basically, we are reading input packet by packet and sending to broker packet by packet in TCP part.\\
\subsection{Broker Node}
We are passing port number of source, router $1$'s IP and port number and router $2$'s IP and port number as argument, since we are going to implement multihoming to make the transaction faster, broker should know where to send which means routers' IP addresses and port numbers. Then in the code we are placing them in necessary places. To see how to execute $broker.py$ file please check $README.md$. In broker basically we are doing 3 operations:

Receiving packet from source

Forming actual packets(packet form for reliable data transfer)

And sending it to destination via appropriate router\\
While receiving packet we need to open appropriate socket connection with source node (TCP connection), with:

self.localSocket = socket.socket(socket.AF\_INET,\\
socket.SOCK\_STREAM)\\
After creating successful connection between source, broker is accepting the packets in recv() function. As soon as the recv() function called, it construct the object which sends packets to destination with sender's window size and set time-out (depends on RTT of applying topology) value for receiver socket, and after, we are writing that packets to $output.txt$ just for manually check the transmission is successful. Since we are sending $996$ $(MAX\_PACKET\_SIZE - 4)$ bytes (MAX\_PACKET\_SIZE depends on designer of applying topology) bytes from source, naturally we are accepting the packets in same length. We have used flags(basic integer variables) in our protocol design to check the conditions in the code flow. In the broker, the flag values are:

flag = $0$ is for first packet

flag = $1$ is for rest of the packets

flag = $2$ is for last packet\\
Based on the flag values, we can track the packets basically. We have implemented multihoming in broker. So, broker picks one of the router and send the packet to destination across to that router. To make decision we get a random value between $0$ and $1$. And if the random number is in interval $[0,0.5)$, broker picks router $1$, and if random number is in interval $[0.5, 1]$, broker picks other router. As stated in Common Parts for All Nodes sub-section, we need to design a protocol that should be reusable for any given topology or configuration, that's why we have implemented the decision of routers by getting random variable. In the future, if someone decides to change the number of routers, the modification can be done easily by changing the interval for routers. After picking router to send via, we start a thread for it, because our protocol is pipe-lined, i.e. to send another packet, our program won't wait to finish current packet sending process. After starting threads, we have joined them so, main thread will wait our threads to finish their job before halting the program. Each thread is working same but with different parameters. Below implementation of SendToDest class' is similar for each threads. While sending SendToDest class has send() function. When a packet arrives to broker from source the process for send() starts (it is not waiting all packets for arrive to broker from source since the connection between source and broker nodes are TCP based, there won't be any package loss). In that process, broker mark all received packets as unacked and form packet according to checksum, sequence number, flag and data values in makePacket() function. Packet consists given sequence number as $2$ byte, checksum as $1$ byte, flag as $1$ byte, and the rest is our data. We calculate the checksum value by taking bitwise-and value of sum of our data and $0xff$ to get unsigned value, to avoid errors  arising from negative checksum values. In makePacket() function, we are converting the values to bytes and since we are going to design a topology or configuration independent protocol, we have indicated the byte values as 'little'. Because the machines that is going to implement our protocol may have different encoding like big-endian, so to make this encoding same for any machine we have stated byte-order as little-endian. After we have implemented our reliable data transfer method to our protocol, Go-Back-N. Our program waits ACK for unacked packets and check if unacked packets' count is greater than the window size or current packet is the last packet or there is still unacked packets that is waiting to be Acked, if so receive Ack from destination, after that if time-out occurs, it goes back N, prints "Go-Back-N"(just for visiualization for us indicating that Go-Back-N mechanism works) and send all N packets again with sendAll() function which sends all packets given. If not, it parses the packets with parsePacket() function, which parse packets contents and calculate new checksum from contents as receivedSum and returns. After that if checksums match we are setting a cumulative Ack number and remove them from unacked list, which increase our protocols' performance since process cumulative Acked packets instead of processing each packet. Then we have arranged the unAcked and lastAcked variables accordingly. While sending packet to destination, sendPacket() function works, sending packet to destination and adjust nextSequnceNumber then starting time-out value (depends on RTT of applying topology) for it's purpose. After all packets are received from source and transmitted to destination we closes the connection but since we are using threads, we separated the connection closing in a function to avoid main thread to close connection while other threads still in progress or even reading data.
\subsection{Destination Node}
In destination class, we also have $2$ threads for multihoming. We have ACK and NACK variables to see the counts of Acks and nAcks. When we receive packet in destination node, we parse the packet and to validate, compare the checksum with calculated checksum (rsum) and sequence numbers. Same properties apply for functions makePacket() and parsePacket() in broker node. When checksum or sequenceNumber checks fail, we create an nAck packet for erroneous packet and send it back, otherwise we send Ack packet and adjust sequence number. In destination class, when last packet arrived, we get the time from \href{https://developers.google.com/time/}{NTP Server of Google} again to synchronization of the nodes and save the time. After that same closing procedure as in broker applies to shutting down the connection. Note that, the packets for Acks and nAcks have sequence number as $2$ bytes, checksum as $1$ byte, flag as $1$ byte and the rest is our empty data.
\newpage